{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "from utils.dataset import BasicDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ckpts_dir/axial_ckpts/  already exists\n"
     ]
    }
   ],
   "source": [
    "dir_train_img = '/home/natasha/unet4/axial_data/train/Fifth_3/pictures/'\n",
    "dir_train_mask = '/home/natasha/unet4/axial_data/train/Fifth_3/masks/'\n",
    "\n",
    "dir_val_img = '/home/natasha/unet4/axial_data/val/Fifth_2/pictures/'\n",
    "dir_val_mask = '/home/natasha/unet4/axial_data/val/Fifth_2/masks/'\n",
    "\n",
    "dir_checkpoint = 'ckpts_dir/axial_ckpts/'\n",
    "\n",
    "try:\n",
    "# Create target Directory\n",
    "    os.mkdir(dir_checkpoint)\n",
    "    print(\"Directory \" , dir_checkpoint , \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dir_checkpoint , \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              val_percent=0.1,\n",
    "              save_cp=True,\n",
    "              epoch_bias=0):\n",
    "\n",
    "    train_dataset = BasicDataset(dir_train_img, dir_train_mask)\n",
    "    val_dataset = BasicDataset(dir_val_img, dir_val_mask)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n",
    "    global_step = 0\n",
    "    val_score_max = 0\n",
    "    n_train = len(train_dataset)\n",
    "    n_val = len(val_dataset)\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "    ''')\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 2 else 'max', patience=2)\n",
    "    if net.n_classes > 2:\n",
    "        print(\"Using CrossEntropyLoss\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "#         print(\"Using NLL loss\")\n",
    "#         criterion = nn.NLLLoss()\n",
    "    else:\n",
    "        print(\"Using BCEWithLogitsLoss\")\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                \n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes <= 10 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                \n",
    "#                 print(f'masks_pred.size() = {masks_pred.size()}')\n",
    "#                 print(f'F.log_softmax(masks_pred, 1).size() = {F.log_softmax(masks_pred, 1).size()}')\n",
    "#                 print(f'true_masks.cpu().numpy() == {true_masks.cpu().numpy()}')\n",
    "#                 print(f'true_masks.size() = {true_masks.size()}')\n",
    "#                 print(f'mask_from_onehot(true_masks) = {mask_from_onehot(true_masks)}')\n",
    "#                 assert 1 == 2\n",
    "                loss = criterion(masks_pred, true_masks.type_as(masks_pred))\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                if global_step % ((n_train + n_val) // (2 * batch_size)) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 2:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_score = eval_net(net, val_loader, device)\n",
    "                \n",
    "            if val_score_max < val_score:\n",
    "                val_score_max = val_score\n",
    "                torch.save(net.state_dict(),\n",
    "                           dir_checkpoint + f'best_epoch_{epoch + epoch_bias + 1}.pth')\n",
    "                logging.info(f'Checkpoint {epoch + epoch_bias + 1} saved !')\n",
    "                logging.info(f'Current max val_score = {val_score_max}')\n",
    "            \n",
    "#             if (epoch + epoch_bias + 1) % 50 == 0:\n",
    "#                 torch.save(net.state_dict(),\n",
    "#                            dir_checkpoint + f'CP_epoch{epoch + epoch_bias + 1}.pth')\n",
    "#                 logging.info(f'Checkpoint {epoch + epoch_bias + 1} saved !')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t7 output channels (classes)\n",
      "\tBilinear upscaling\n"
     ]
    }
   ],
   "source": [
    "net = UNet(n_channels=1, n_classes=7, bilinear=True)\n",
    "logging.info(f'Network:\\n'\n",
    "             f'\\t{net.n_channels} input channels\\n'\n",
    "             f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "             f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Creating dataset with 100 examples\n",
      "INFO: Creating dataset with 100 examples\n",
      "INFO: Starting training:\n",
      "        Epochs:          3\n",
      "        Batch size:      10\n",
      "        Learning rate:   0.001\n",
      "        Training size:   100\n",
      "        Validation size: 100\n",
      "        Checkpoints:     True\n",
      "        Device:          cuda\n",
      "        Images scaling:  1\n",
      "    \n",
      "Epoch 1/3:   0%|          | 0/100 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CrossEntropyLoss\n",
      "image shape = (308, 320)image shape = (308, 320)image shape = (308, 320)image shape = (308, 320)image shape = (308, 320)\n",
      "\n",
      "\n",
      "image shape = (308, 320)image shape = (308, 320)image shape = (308, 320)\n",
      "\n",
      "image shape = (308, 320)\n",
      "image shape = (308, 320)\n",
      "\n",
      "\n",
      "image shape = (308, 320)\n",
      "image shape = (308, 320)image shape = (308, 320)\n",
      "\n",
      "image shape = (300, 320)image shape = (308, 320)image shape = (308, 320)\n",
      "\n",
      "image shape = (308, 320)\n",
      "image shape = (308, 320)image shape = (308, 320)\n",
      "\n",
      "\n",
      "image shape = (308, 320)image shape = (308, 320)\n",
      "image shape = (308, 320)\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# load = dir_checkpoint+'CP_epoch400.pth'\n",
    "load = False\n",
    "# epoch_bias = 400\n",
    "\n",
    "if load:\n",
    "    net.load_state_dict(\n",
    "        torch.load(load, map_location=device)\n",
    "    )\n",
    "    logging.info(f'Model loaded from {load}')\n",
    "\n",
    "net.to(device=device)\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "try:\n",
    "    train_net(net=net,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              device=device)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
