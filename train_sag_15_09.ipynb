{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.dataset_old import BasicDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ckpts_dir/sag_ckpts/  already exists\n"
     ]
    }
   ],
   "source": [
    "dir_img = '/home/vlad/dataset_new/train_data/imgs/'\n",
    "dir_mask = '/home/vlad/dataset_new/train_data/masks/'\n",
    "\n",
    "dir_checkpoint = 'ckpts_dir/sag_ckpts/'\n",
    "\n",
    "try:\n",
    "# Create target Directory\n",
    "    os.mkdir(dir_checkpoint)\n",
    "    print(\"Directory \" , dir_checkpoint ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dir_checkpoint ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              val_percent=0.1,\n",
    "              save_cp=True,\n",
    "              img_scale=0.5,\n",
    "              epoch_bias=0):\n",
    "\n",
    "    dataset = BasicDataset(dir_img, dir_mask, img_scale)\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train, val = random_split(dataset, [n_train, n_val])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n",
    "    global_step = 0\n",
    "    val_score_max = 0\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "    ''')\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 2 else 'max', patience=2)\n",
    "    if net.n_classes > 2:\n",
    "        print(\"Using CrossEntropyLoss\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        print(\"Using BCEWithLogitsLoss\")\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                \n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes <= 2 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                \n",
    "                print(f'masks_pred.size() = {masks_pred.size()}')\n",
    "                print(f'true_masks.size() = {true_masks.size()}')\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                if global_step % (len(dataset) // (2 * batch_size)) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 2:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_score = eval_net(net, val_loader, device)\n",
    "                \n",
    "            if val_score_max < val_score:\n",
    "                val_score_max = val_score\n",
    "                torch.save(net.state_dict(),\n",
    "                           dir_checkpoint + f'best_epoch_{epoch + epoch_bias + 1}.pth')\n",
    "                logging.info(f'Checkpoint {epoch + epoch_bias + 1} saved !')\n",
    "                logging.info(f'Current max val_score = {val_score_max}')\n",
    "            \n",
    "#             if (epoch + epoch_bias + 1) % 50 == 0:\n",
    "#                 torch.save(net.state_dict(),\n",
    "#                            dir_checkpoint + f'CP_epoch{epoch + epoch_bias + 1}.pth')\n",
    "#                 logging.info(f'Checkpoint {epoch + epoch_bias + 1} saved !')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t2 output channels (classes)\n",
      "\tBilinear upscaling\n"
     ]
    }
   ],
   "source": [
    "net = UNet(n_channels=1, n_classes=2, bilinear=True)\n",
    "logging.info(f'Network:\\n'\n",
    "             f'\\t{net.n_channels} input channels\\n'\n",
    "             f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "             f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Creating dataset with 1288 examples\n",
      "INFO: Starting training:\n",
      "        Epochs:          600\n",
      "        Batch size:      4\n",
      "        Learning rate:   0.001\n",
      "        Training size:   1160\n",
      "        Validation size: 128\n",
      "        Checkpoints:     True\n",
      "        Device:          cuda\n",
      "        Images scaling:  0.5\n",
      "    \n",
      "Epoch 1/600:   0%|          | 0/1160 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BCEWithLogitsLoss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   1%|          | 8/1160 [00:00<01:05, 17.58img/s, loss (batch)=0.619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   1%|▏         | 16/1160 [00:00<00:51, 22.06img/s, loss (batch)=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   2%|▏         | 24/1160 [00:00<00:44, 25.31img/s, loss (batch)=0.0969]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   3%|▎         | 32/1160 [00:01<00:41, 27.25img/s, loss (batch)=0.0641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   3%|▎         | 40/1160 [00:01<00:39, 28.30img/s, loss (batch)=0.0481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   4%|▍         | 48/1160 [00:01<00:38, 28.76img/s, loss (batch)=0.0456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   5%|▍         | 56/1160 [00:02<00:37, 29.16img/s, loss (batch)=0.0505]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   6%|▌         | 64/1160 [00:02<00:37, 29.28img/s, loss (batch)=0.0465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   6%|▌         | 72/1160 [00:02<00:37, 29.33img/s, loss (batch)=0.0391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   7%|▋         | 80/1160 [00:02<00:36, 29.37img/s, loss (batch)=0.041] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   8%|▊         | 88/1160 [00:03<00:36, 29.38img/s, loss (batch)=0.0445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   8%|▊         | 96/1160 [00:03<00:36, 29.40img/s, loss (batch)=0.0496]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   9%|▉         | 104/1160 [00:03<00:35, 29.41img/s, loss (batch)=0.0445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:  10%|▉         | 112/1160 [00:03<00:35, 29.39img/s, loss (batch)=0.0407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:  10%|█         | 120/1160 [00:04<00:35, 29.43img/s, loss (batch)=0.0425]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n",
      "masks_pred.size() = torch.Size([4, 2, 275, 275])\n",
      "true_masks.size() = torch.Size([4, 2, 275, 275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:  10%|█         | 120/1160 [00:04<00:37, 27.77img/s, loss (batch)=0.0425]\n",
      "INFO: Saved interrupt\n"
     ]
    }
   ],
   "source": [
    "epochs = 600\n",
    "batch_size = 4\n",
    "n_classes = 2 \n",
    "n_channels = 1\n",
    "\n",
    "# load = dir_checkpoint+'CP_epoch400.pth'\n",
    "load = False\n",
    "# epoch_bias = 400\n",
    "\n",
    "if load:\n",
    "    net.load_state_dict(\n",
    "        torch.load(load, map_location=device)\n",
    "    )\n",
    "    logging.info(f'Model loaded from {load}')\n",
    "\n",
    "net.to(device=device)\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "try:\n",
    "    train_net(net=net,\n",
    "              \n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              device=device)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
