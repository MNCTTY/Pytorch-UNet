{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.dataset import BasicDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ckpts_dir/mid_ckpts/  already exists\n"
     ]
    }
   ],
   "source": [
    "dir_img = '/home/ubuntu/T2_Sag_Mid_new/train_data/imgs/'\n",
    "dir_mask = '/home/ubuntu/T2_Sag_Mid_new/train_data/masks/'\n",
    "\n",
    "dir_checkpoint = 'ckpts_dir/mid_ckpts/'\n",
    "\n",
    "try:\n",
    "# Create target Directory\n",
    "    os.mkdir(dir_checkpoint)\n",
    "    print(\"Directory \" , dir_checkpoint ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dir_checkpoint ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.001,\n",
    "              val_percent=0.1,\n",
    "              save_cp=True,\n",
    "              img_scale=0.5,\n",
    "              epoch_bias = 0):\n",
    "\n",
    "    dataset = BasicDataset(dir_img, dir_mask, img_scale)\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train, val = random_split(dataset, [n_train, n_val])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n",
    "    global_step = 0\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "    ''')\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 2 else 'max', patience=2)\n",
    "    if net.n_classes > 2:\n",
    "        print(\"Using CrossEntropyLoss\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        print(\"Using BCEWithLogitsLoss\")\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                \n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes <= 2 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                \n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                if global_step % (len(dataset) // (10 * batch_size)) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 2:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            if (epoch + epoch_bias + 1) % 50 == 0:\n",
    "                torch.save(net.state_dict(),\n",
    "                           dir_checkpoint + f'CP_epoch{epoch + epoch_bias + 1}.pth')\n",
    "                logging.info(f'Checkpoint {epoch + epoch_bias + 1} saved !')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t2 output channels (classes)\n",
      "\tBilinear upscaling\n"
     ]
    }
   ],
   "source": [
    "net = UNet(n_channels=1, n_classes=2, bilinear=True)\n",
    "logging.info(f'Network:\\n'\n",
    "             f'\\t{net.n_channels} input channels\\n'\n",
    "             f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "             f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2 \n",
    "n_channels = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100 \n",
    "# batchsize = 16\n",
    "\n",
    "# load = False\n",
    "# epoch_bias = 0\n",
    "\n",
    "# if load:\n",
    "#     net.load_state_dict(\n",
    "#         torch.load(load, map_location=device)\n",
    "#     )\n",
    "#     logging.info(f'Model loaded from {load}')\n",
    "\n",
    "# net.to(device=device)\n",
    "\n",
    "# # clear_output()\n",
    "\n",
    "# try:\n",
    "#     train_net(net=net,\n",
    "#               epochs=epochs,\n",
    "#               batch_size=batchsize,\n",
    "#               device=device,\n",
    "#               epoch_bias=epoch_bias)\n",
    "# except KeyboardInterrupt:\n",
    "#     torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "#     logging.info('Saved interrupt')\n",
    "#     try:\n",
    "#         sys.exit(0)\n",
    "#     except SystemExit:\n",
    "#         os._exit(0)\n",
    "        \n",
    "# epochs = 100 \n",
    "# batchsize = 8\n",
    "\n",
    "# load = dir_checkpoint+'CP_epoch100.pth'\n",
    "# epoch_bias = 100\n",
    "\n",
    "# if load:\n",
    "#     net.load_state_dict(\n",
    "#         torch.load(load, map_location=device)\n",
    "#     )\n",
    "#     logging.info(f'Model loaded from {load}')\n",
    "\n",
    "# net.to(device=device)\n",
    "\n",
    "# # clear_output()\n",
    "\n",
    "# try:\n",
    "#     train_net(net=net,\n",
    "#               epochs=epochs,\n",
    "#               batch_size=batchsize,\n",
    "#               device=device,\n",
    "#               epoch_bias=epoch_bias)\n",
    "# except KeyboardInterrupt:\n",
    "#     torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "#     logging.info('Saved interrupt')\n",
    "#     try:\n",
    "#         sys.exit(0)\n",
    "#     except SystemExit:\n",
    "#         os._exit(0)\n",
    "        \n",
    "# epochs = 50 \n",
    "# batchsize = 4\n",
    "\n",
    "# load = dir_checkpoint+'CP_epoch200.pth'\n",
    "# epoch_bias = 200\n",
    "\n",
    "# if load:\n",
    "#     net.load_state_dict(\n",
    "#         torch.load(load, map_location=device)\n",
    "#     )\n",
    "#     logging.info(f'Model loaded from {load}')\n",
    "\n",
    "# net.to(device=device)\n",
    "\n",
    "# # clear_output()\n",
    "\n",
    "# try:\n",
    "#     train_net(net=net,\n",
    "#               epochs=epochs,\n",
    "#               batch_size=batchsize,\n",
    "#               device=device,\n",
    "#               epoch_bias=epoch_bias)\n",
    "# except KeyboardInterrupt:\n",
    "#     torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "#     logging.info('Saved interrupt')\n",
    "#     try:\n",
    "#         sys.exit(0)\n",
    "#     except SystemExit:\n",
    "#         os._exit(0)\n",
    "        \n",
    "# epochs = 50 \n",
    "# batchsize = 2\n",
    "\n",
    "# load = dir_checkpoint+'CP_epoch250.pth'\n",
    "# epoch_bias = 250\n",
    "\n",
    "# if load:\n",
    "#     net.load_state_dict(\n",
    "#         torch.load(load, map_location=device)\n",
    "#     )\n",
    "#     logging.info(f'Model loaded from {load}')\n",
    "\n",
    "# net.to(device=device)\n",
    "\n",
    "# # clear_output()\n",
    "\n",
    "# try:\n",
    "#     train_net(net=net,\n",
    "#               epochs=epochs,\n",
    "#               batch_size=batchsize,\n",
    "#               device=device,\n",
    "#               epoch_bias=epoch_bias)\n",
    "# except KeyboardInterrupt:\n",
    "#     torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "#     logging.info('Saved interrupt')\n",
    "#     try:\n",
    "#         sys.exit(0)\n",
    "#     except SystemExit:\n",
    "#         os._exit(0)\n",
    "        \n",
    "# epochs = 100 \n",
    "# batchsize = 1\n",
    "\n",
    "# load = dir_checkpoint+'CP_epoch300.pth'\n",
    "# epoch_bias = 300\n",
    "\n",
    "# if load:\n",
    "#     net.load_state_dict(\n",
    "#         torch.load(load, map_location=device)\n",
    "#     )\n",
    "#     logging.info(f'Model loaded from {load}')\n",
    "\n",
    "# net.to(device=device)\n",
    "\n",
    "# # clear_output()\n",
    "\n",
    "# try:\n",
    "#     train_net(net=net,\n",
    "#               epochs=epochs,\n",
    "#               batch_size=batchsize,\n",
    "#               device=device,\n",
    "#               epoch_bias=epoch_bias)\n",
    "# except KeyboardInterrupt:\n",
    "#     torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "#     logging.info('Saved interrupt')\n",
    "#     try:\n",
    "#         sys.exit(0)\n",
    "#     except SystemExit:\n",
    "#         os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Model loaded from ckpts_dir/mid_ckpts/CP_epoch400.pth\n",
      "INFO: Creating dataset with 618 examples\n",
      "INFO: Starting training:\n",
      "        Epochs:          300\n",
      "        Batch size:      1\n",
      "        Learning rate:   0.001\n",
      "        Training size:   557\n",
      "        Validation size: 61\n",
      "        Checkpoints:     True\n",
      "        Device:          cuda\n",
      "        Images scaling:  0.5\n",
      "    \n",
      "Epoch 1/300:   0%|          | 0/557 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BCEWithLogitsLoss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300:  11%|█         | 61/557 [00:04<00:37, 13.18img/s, loss (batch)=0.00828]\n",
      "Validation round:   0%|          | 0/61 [00:00<?, ?batch/s]\u001b[A\n",
      "Validation round:   2%|▏         | 1/61 [00:00<00:14,  4.20batch/s]\u001b[A\n",
      "Validation round:   5%|▍         | 3/61 [00:00<00:10,  5.49batch/s]\u001b[A\n",
      "Validation round:  11%|█▏        | 7/61 [00:00<00:07,  7.33batch/s]\u001b[A\n",
      "Validation round:  18%|█▊        | 11/61 [00:00<00:05,  9.63batch/s]\u001b[A\n",
      "Validation round:  26%|██▌       | 16/61 [00:00<00:03, 12.58batch/s]\u001b[A\n",
      "Validation round:  34%|███▍      | 21/61 [00:00<00:02, 15.98batch/s]\u001b[A\n",
      "Validation round:  43%|████▎     | 26/61 [00:00<00:01, 19.68batch/s]\u001b[A\n",
      "Validation round:  51%|█████     | 31/61 [00:01<00:01, 23.55batch/s]\u001b[A\n",
      "Validation round:  59%|█████▉    | 36/61 [00:01<00:00, 27.29batch/s]\u001b[A\n",
      "Validation round:  67%|██████▋   | 41/61 [00:01<00:00, 30.65batch/s]\u001b[A\n",
      "Validation round:  75%|███████▌  | 46/61 [00:01<00:00, 33.60batch/s]\u001b[A\n",
      "Validation round:  84%|████████▎ | 51/61 [00:01<00:00, 36.08batch/s]\u001b[A\n",
      "Validation round:  92%|█████████▏| 56/61 [00:01<00:00, 37.95batch/s]\u001b[A\n",
      "Validation round: 100%|██████████| 61/61 [00:01<00:00, 39.47batch/s]\u001b[A\n",
      "                                                                    \u001b[AINFO: Validation Dice Coeff: 0.8963929326807867\n",
      "Epoch 1/300:  22%|██▏       | 121/557 [00:14<00:33, 13.10img/s, loss (batch)=0.00378]\n",
      "Validation round:   0%|          | 0/61 [00:00<?, ?batch/s]\u001b[A\n",
      "Validation round:   2%|▏         | 1/61 [00:00<00:14,  4.00batch/s]\u001b[A\n",
      "Validation round:   7%|▋         | 4/61 [00:00<00:10,  5.36batch/s]\u001b[A\n",
      "Validation round:  13%|█▎        | 8/61 [00:00<00:07,  7.17batch/s]\u001b[A\n",
      "Validation round:  21%|██▏       | 13/61 [00:00<00:05,  9.57batch/s]\u001b[A\n",
      "Validation round:  30%|██▉       | 18/61 [00:00<00:03, 12.49batch/s]\u001b[A\n",
      "Validation round:  38%|███▊      | 23/61 [00:00<00:02, 15.87batch/s]\u001b[A\n",
      "Validation round:  46%|████▌     | 28/61 [00:00<00:01, 19.62batch/s]\u001b[A\n",
      "Validation round:  54%|█████▍    | 33/61 [00:01<00:01, 23.49batch/s]\u001b[A\n",
      "Validation round:  62%|██████▏   | 38/61 [00:01<00:00, 27.21batch/s]\u001b[A\n",
      "Validation round:  70%|███████   | 43/61 [00:01<00:00, 30.58batch/s]\u001b[A\n",
      "Validation round:  79%|███████▊  | 48/61 [00:01<00:00, 33.59batch/s]\u001b[A\n",
      "Validation round:  87%|████████▋ | 53/61 [00:01<00:00, 36.06batch/s]\u001b[A\n",
      "Validation round:  95%|█████████▌| 58/61 [00:01<00:00, 37.91batch/s]\u001b[A\n",
      "                                                                    \u001b[AINFO: Validation Dice Coeff: 0.9077529995167841\n",
      "Epoch 1/300:  33%|███▎      | 183/557 [00:24<00:28, 13.05img/s, loss (batch)=0.00777]\n",
      "Validation round:   0%|          | 0/61 [00:00<?, ?batch/s]\u001b[A\n",
      "Validation round:   2%|▏         | 1/61 [00:00<00:13,  4.50batch/s]\u001b[A\n",
      "Validation round:   7%|▋         | 4/61 [00:00<00:09,  5.97batch/s]\u001b[A\n",
      "Validation round:  13%|█▎        | 8/61 [00:00<00:06,  7.97batch/s]\u001b[A\n",
      "Validation round:  21%|██▏       | 13/61 [00:00<00:04, 10.55batch/s]\u001b[A\n",
      "Validation round:  30%|██▉       | 18/61 [00:00<00:03, 13.64batch/s]\u001b[A\n",
      "Validation round:  38%|███▊      | 23/61 [00:00<00:02, 17.15batch/s]\u001b[A\n",
      "Validation round:  46%|████▌     | 28/61 [00:00<00:01, 20.92batch/s]\u001b[A\n",
      "Validation round:  54%|█████▍    | 33/61 [00:01<00:01, 24.72batch/s]\u001b[A\n",
      "Validation round:  62%|██████▏   | 38/61 [00:01<00:00, 28.37batch/s]\u001b[A\n",
      "Validation round:  70%|███████   | 43/61 [00:01<00:00, 31.56batch/s]\u001b[A\n",
      "Validation round:  79%|███████▊  | 48/61 [00:01<00:00, 34.25batch/s]\u001b[A\n",
      "Validation round:  87%|████████▋ | 53/61 [00:01<00:00, 36.51batch/s]\u001b[A\n",
      "Validation round:  95%|█████████▌| 58/61 [00:01<00:00, 38.24batch/s]\u001b[A\n",
      "                                                                    \u001b[AINFO: Validation Dice Coeff: 0.7182217377131103\n",
      "Epoch 1/300:  44%|████▎     | 243/557 [00:33<00:24, 13.01img/s, loss (batch)=0.0106] \n",
      "Validation round:   0%|          | 0/61 [00:00<?, ?batch/s]\u001b[A\n",
      "Validation round:   2%|▏         | 1/61 [00:00<00:14,  4.21batch/s]\u001b[A\n",
      "Validation round:   7%|▋         | 4/61 [00:00<00:10,  5.61batch/s]\u001b[A\n",
      "Validation round:  11%|█▏        | 7/61 [00:00<00:07,  7.35batch/s]\u001b[A\n",
      "Validation round:  20%|█▉        | 12/61 [00:00<00:05,  9.79batch/s]\u001b[A\n",
      "Validation round:  28%|██▊       | 17/61 [00:00<00:03, 12.75batch/s]\u001b[A\n",
      "Validation round:  36%|███▌      | 22/61 [00:00<00:02, 16.13batch/s]\u001b[A\n",
      "Validation round:  44%|████▍     | 27/61 [00:00<00:01, 19.84batch/s]\u001b[A\n",
      "Validation round:  52%|█████▏    | 32/61 [00:01<00:01, 23.63batch/s]\u001b[A\n",
      "Validation round:  61%|██████    | 37/61 [00:01<00:00, 27.24batch/s]\u001b[A\n",
      "Validation round:  69%|██████▉   | 42/61 [00:01<00:00, 30.57batch/s]\u001b[A\n",
      "Validation round:  77%|███████▋  | 47/61 [00:01<00:00, 33.42batch/s]\u001b[A\n",
      "Validation round:  85%|████████▌ | 52/61 [00:01<00:00, 35.84batch/s]\u001b[A\n",
      "Validation round:  93%|█████████▎| 57/61 [00:01<00:00, 37.72batch/s]\u001b[A\n",
      "                                                                    \u001b[AINFO: Validation Dice Coeff: 0.9031027535923192\n",
      "Epoch 1/300:  46%|████▋     | 259/557 [00:39<00:41,  7.22img/s, loss (batch)=0.00617]"
     ]
    }
   ],
   "source": [
    "epochs = 300 \n",
    "batchsize = 1\n",
    "\n",
    "load = dir_checkpoint+'CP_epoch400.pth'\n",
    "epoch_bias = 400\n",
    "\n",
    "if load:\n",
    "    net.load_state_dict(\n",
    "        torch.load(load, map_location=device)\n",
    "    )\n",
    "    logging.info(f'Model loaded from {load}')\n",
    "\n",
    "net.to(device=device)\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "try:\n",
    "    train_net(net=net,\n",
    "              epochs=epochs,\n",
    "              batch_size=batchsize,\n",
    "              device=device,\n",
    "              epoch_bias=epoch_bias)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
